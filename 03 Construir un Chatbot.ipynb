{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar variables de entorno desde .env\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv()  # Esto cargará las variables definidas en el archivo .env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4CYcHxo4Y7zh"
   },
   "source": [
    "# Construir un Chatbot\n",
    "\n",
    "# **Descripción general**\n",
    "\n",
    "Revisaremos un ejemplo de cómo diseñar e implementar un chatbot con motor LLM. Este chatbot podrá tener una conversación y recordar interacciones anteriores con un [modelo de chat](https://python.langchain.com/docs/concepts/chat_models/) .\n",
    "\n",
    "Tenga en cuenta que este chatbot que construiremos solo usará el modelo del lenguaje para tener una conversación. Hay varios otros conceptos relacionados que puede estar buscando:\n",
    "\n",
    "- [Conversación RAG](https://python.langchain.com/docs/tutorials/qa_chat_history/): Habilitar una experiencia de chatbot en la que se pueda conectar con una fuente externa de datos para producir respuestas basadas en ella.\n",
    "- [Agentes](https://python.langchain.com/docs/tutorials/agents/) : Construir un chatbot que pueda llevar a cabo acciones.\n",
    "\n",
    "Este tutorial cubrirá los conceptos básicos que serán útiles para esos dos temas más avanzados.\n",
    "\n",
    "# **Instalación**\n",
    "\n",
    "Para este tutorial necesitaremos `langchain-core` y `langgraph`. Esta guía requiere `langgraph >= 0.2.28`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 16277,
     "status": "ok",
     "timestamp": 1752739133959,
     "user": {
      "displayName": "Aitor Donado",
      "userId": "08246046509718212083"
     },
     "user_tz": -120
    },
    "id": "XnIs4gRkZEdo"
   },
   "outputs": [],
   "source": [
    "!pip install langchain-core langgraph>0.2.27"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q9th9rNuY7zm"
   },
   "source": [
    "\n",
    "\n",
    "# **LangSmith**\n",
    "\n",
    "Muchas de las aplicaciones que construye con Langchain contendrán múltiples pasos con múltiples invocaciones de llamadas LLM. A medida que estas aplicaciones se vuelven cada vez más complejas, se vuelve crucial poder inspeccionar lo que está sucediendo exactamente dentro de su cadena o agente. La mejor manera de hacer esto es con [LangSmith](https://smith.langchain.com/).\n",
    "\n",
    "Después de registrarse en el enlace de arriba, asegúrese de establecer sus variables de entorno para comenzar a registrar las trazas:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_gkPjzoYHACx"
   },
   "source": [
    "```\n",
    "export LANGSMITH_TRACING=\"true\"\n",
    "export LANGSMITH_API_KEY=\"...\"\n",
    "export LANGSMITH_PROJECT=\"default\" # or any other project name\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fTgqRZa0HACx"
   },
   "source": [
    "\n",
    "\n",
    "O, si en un cuaderno, puede configurarlos con:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6151,
     "status": "ok",
     "timestamp": 1752739222174,
     "user": {
      "displayName": "Aitor Donado",
      "userId": "08246046509718212083"
     },
     "user_tz": -120
    },
    "id": "MuDAwrkdHACy",
    "outputId": "ccb6340a-5d81-4f70-af06-f6a5bffccd48"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter your LangSmith API key (optional): ··········\n",
      "Enter your LangSmith Project Name (default = \"pr-glossy-thought-54\"): ··········\n"
     ]
    }
   ],
   "source": [
    "import getpass\n",
    "import os\n",
    "try:\n",
    "    # load environment variables from .env file (requires `python-dotenv`)\n",
    "    from dotenv import load_dotenv\n",
    "    load_dotenv()\n",
    "except ImportError:\n",
    "    pass\n",
    "os.environ[\"LANGSMITH_TRACING\"] = \"true\"\n",
    "if \"LANGSMITH_API_KEY\" not in os.environ:\n",
    "    os.environ[\"LANGSMITH_API_KEY\"] = getpass.getpass(\n",
    "        prompt=\"Enter your LangSmith API key (optional): \"\n",
    "    )\n",
    "if \"LANGSMITH_PROJECT\" not in os.environ:\n",
    "    os.environ[\"LANGSMITH_PROJECT\"] = getpass.getpass(\n",
    "        prompt='Enter your LangSmith Project Name (default = \"pr-glossy-thought-54\"): '\n",
    "    )\n",
    "    if not os.environ.get(\"LANGSMITH_PROJECT\"):\n",
    "        os.environ[\"LANGSMITH_PROJECT\"] = \"pr-glossy-thought-54\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GRzGu7T7Y7zo"
   },
   "source": [
    "\n",
    "\n",
    "# **Inicio rápido**\n",
    "\n",
    "Primero, aprendamos a usar un modelo de idioma por sí mismo. Langchain admite muchos modelos de idioma diferentes que puede usar indistintamente: ¡seleccione el que desea usar a continuación!\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7275,
     "status": "ok",
     "timestamp": 1752739289903,
     "user": {
      "displayName": "Aitor Donado",
      "userId": "08246046509718212083"
     },
     "user_tz": -120
    },
    "id": "SKZX-DgeU6k3",
    "outputId": "f7716db3-2915-4938-b7f9-0db03bf8de11"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/131.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m131.1/131.1 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "%pip install -qU langchain-groq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6065,
     "status": "ok",
     "timestamp": 1752739326902,
     "user": {
      "displayName": "Aitor Donado",
      "userId": "08246046509718212083"
     },
     "user_tz": -120
    },
    "id": "c8Dt8nj1HAC0",
    "outputId": "c43b6407-9d0e-42b4-e46f-310af51df445"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter API key for Groq: ··········\n"
     ]
    }
   ],
   "source": [
    "import getpass\n",
    "import os\n",
    "if not os.environ.get(\"GROQ_API_KEY\"):\n",
    "  os.environ[\"GROQ_API_KEY\"] = getpass.getpass(\"Enter API key for Groq: \")\n",
    "from langchain.chat_models import init_chat_model\n",
    "model = init_chat_model(\"llama3-8b-8192\", model_provider=\"groq\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oce3taDHY7zp"
   },
   "source": [
    "\n",
    "\n",
    "Primero usemos el modelo directamente. Los [ChatModels](https://python.langchain.com/docs/concepts/chat_models/) son instancias de [Runnables](https://python.langchain.com/docs/concepts/runnables/) de LangChain, lo que significa que exponen una interfaz estándar para interactuar con ellos. Para simplemente llamar al modelo, podemos pasar en una lista de [mensajes](https://python.langchain.com/docs/concepts/messages/) hacia el método `.invoke`.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 376,
     "status": "ok",
     "timestamp": 1752739346852,
     "user": {
      "displayName": "Aitor Donado",
      "userId": "08246046509718212083"
     },
     "user_tz": -120
    },
    "id": "_W3R6oZmY7zp",
    "outputId": "f896d07a-d8cc-409d-ea41-27578a3afb13"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Hi Bob! Nice to meet you! Is there something I can help you with or would you like to chat?', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 24, 'prompt_tokens': 15, 'total_tokens': 39, 'completion_time': 0.022352034, 'prompt_time': 0.002563687, 'queue_time': 0.063762491, 'total_time': 0.024915721}, 'model_name': 'llama3-8b-8192', 'system_fingerprint': 'fp_8dc6ecaf8e', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--25b7acfb-cfde-4e61-845e-8292d8f2ba02-0', usage_metadata={'input_tokens': 15, 'output_tokens': 24, 'total_tokens': 39})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "model.invoke([HumanMessage(content=\"Hi! I'm Bob\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XjyuirXFY7zq"
   },
   "source": [
    "\n",
    "\n",
    "**Referencia de API:** [HumanMessage](https://python.langchain.com/api_reference/core/messages/langchain_core.messages.human.HumanMessage.html)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xBZfqXdxY7zq"
   },
   "source": [
    "\n",
    "\n",
    "El modelo por sí solo no tiene ningún concepto de estado. Por ejemplo, si hace una pregunta de seguimiento:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 84,
     "status": "ok",
     "timestamp": 1752739358905,
     "user": {
      "displayName": "Aitor Donado",
      "userId": "08246046509718212083"
     },
     "user_tz": -120
    },
    "id": "2l-vTLTqY7zq",
    "outputId": "f594d485-08e3-411d-d10a-67ced4649b62"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"I'm afraid I don't know! We just started chatting, and I don't have any information about you, including your name. Would you like to introduce yourself? I'm happy to learn more about you!\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 44, 'prompt_tokens': 15, 'total_tokens': 59, 'completion_time': 0.093007749, 'prompt_time': 0.007437752, 'queue_time': 0.001516573000000001, 'total_time': 0.100445501}, 'model_name': 'llama3-8b-8192', 'system_fingerprint': 'fp_0fb809dba3', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--4d1ef5e2-7d5a-42f3-b9b3-942597a65d20-0', usage_metadata={'input_tokens': 15, 'output_tokens': 44, 'total_tokens': 59})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.invoke([HumanMessage(content=\"What's my name?\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JGACjybjY7zq"
   },
   "source": [
    "\n",
    "\n",
    "Echemos un vistazo [Trace de LangSmith](https://smith.langchain.com/public/5c21cb92-2814-4119-bae9-d02b8db577ac/r) del ejemplo.\n",
    "\n",
    "Podemos ver que no lleva la conversación anterior como contexto y no puede responder la pregunta. ¡Esto lo convierte en una terrible experiencia en chatbot!\n",
    "\n",
    "Para evitar esto, necesitamos pasar todo el [historial de conversación](https://python.langchain.com/docs/concepts/chat_history/) al modelo. Veamos qué sucede cuando hacemos eso:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 186,
     "status": "ok",
     "timestamp": 1752739436462,
     "user": {
      "displayName": "Aitor Donado",
      "userId": "08246046509718212083"
     },
     "user_tz": -120
    },
    "id": "JCYSRO8DY7zq",
    "outputId": "f7d54a07-3814-409d-9427-66d0191a4252"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='I know this one! Your name is Bob!', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 11, 'prompt_tokens': 40, 'total_tokens': 51, 'completion_time': 0.021509986, 'prompt_time': 0.016684052, 'queue_time': 0.068210126, 'total_time': 0.038194038}, 'model_name': 'llama3-8b-8192', 'system_fingerprint': 'fp_bc17983608', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--f5dc2c53-5309-469f-8d4d-3f41ab5c0934-0', usage_metadata={'input_tokens': 40, 'output_tokens': 11, 'total_tokens': 51})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import AIMessage\n",
    "model.invoke(\n",
    "    [\n",
    "        HumanMessage(content=\"Hi! I'm Bob\"),\n",
    "        AIMessage(content=\"Hello Bob! How can I assist you today?\"),\n",
    "        HumanMessage(content=\"What's my name?\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LqEZP8hpY7zq"
   },
   "source": [
    "**Referencia de API:** [AIMessage](https://python.langchain.com/api_reference/core/messages/langchain_core.messages.ai.AIMessage.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1054UfYqY7zq"
   },
   "source": [
    "\n",
    "\n",
    "¡Y ahora podemos ver que obtenemos una buena respuesta!\n",
    "\n",
    "Esta es la idea básica que sustenta la capacidad de un chatbot para interactuar con conversación. Entonces, ¿cómo implementamos mejor esto?\n",
    "\n",
    "# **Persistencia del mensaje**\n",
    "\n",
    "[LangGraph](https://langchain-ai.github.io/langgraph/) Implementa una capa de persistencia incorporada, lo que lo hace ideal para aplicaciones de chat que admiten múltiples giros conversacionales.\n",
    "\n",
    "Envolver nuestro modelo de chat en una aplicación mínima de LangGraph nos permite persistir automáticamente el historial de mensajes, simplificando el desarrollo de aplicaciones múltiples.\n",
    "\n",
    "LangGraph viene con un simple checkpointer en memoria, que usamos a continuación. Vea su [documentación](https://langchain-ai.github.io/langgraph/concepts/persistence/) Para obtener más detalles, incluyendo cómo usar diferentes backends de persistencia (por ejemplo, SQLite o Postgres).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 16,
     "status": "ok",
     "timestamp": 1752739574401,
     "user": {
      "displayName": "Aitor Donado",
      "userId": "08246046509718212083"
     },
     "user_tz": -120
    },
    "id": "I4FA4b_fY7zq"
   },
   "outputs": [],
   "source": [
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import START, MessagesState, StateGraph\n",
    "\n",
    "# Definir un nuevo grafo\n",
    "workflow = StateGraph(state_schema=MessagesState)\n",
    "\n",
    "# Definir la función que llama al modelo\n",
    "def call_model(state: MessagesState):\n",
    "    response = model.invoke(state[\"messages\"])\n",
    "    return {\"messages\": response}\n",
    "\n",
    "# Definir el nodo (sólo uno) en el grafo\n",
    "workflow.add_edge(START, \"model\")\n",
    "workflow.add_node(\"model\", call_model)\n",
    "\n",
    "# Añadir memoria\n",
    "memory = MemorySaver()\n",
    "app = workflow.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "puGg1CpFY7zq"
   },
   "source": [
    "\n",
    "\n",
    "**Referencia de API:** [MemorySaver](https://langchain-ai.github.io/langgraph/reference/checkpoints/#langgraph.checkpoint.memory.MemorySaver) | [StateGraph](https://langchain-ai.github.io/langgraph/reference/graphs/#langgraph.graph.state.StateGraph)\n",
    "\n",
    "Ahora necesitamos crear una `config`que pasaremos al runnable cada vez. Esta configuración contiene información que no es parte de la entrada directamente, pero que sigue siendo útil. En este caso, queremos incluir un `thread_id`. Esto debería verse como:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "executionInfo": {
     "elapsed": 41,
     "status": "ok",
     "timestamp": 1752743684424,
     "user": {
      "displayName": "Aitor Donado",
      "userId": "08246046509718212083"
     },
     "user_tz": -120
    },
    "id": "EUHxGesxY7zr"
   },
   "outputs": [],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"abc123\"}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "joxS4ho5Y7zr"
   },
   "source": [
    "\n",
    "\n",
    "Esto nos permite admitir múltiples hilos de conversación con una sola aplicación, un requisito común cuando su aplicación tiene múltiples usuarios.\n",
    "\n",
    "Luego podemos invocar la aplicación:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 222,
     "status": "ok",
     "timestamp": 1752743700281,
     "user": {
      "displayName": "Aitor Donado",
      "userId": "08246046509718212083"
     },
     "user_tz": -120
    },
    "id": "_Hcehq75Y7zr",
    "outputId": "3195dc43-d645-4ece-ecfc-d95488b98606"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Hello Bob! It's nice to meet you. Is there something I can help you with or would you like to chat?\n"
     ]
    }
   ],
   "source": [
    "query = \"Hi! I'm Bob.\"\n",
    "input_messages = [HumanMessage(query)]\n",
    "output = app.invoke({\"messages\": input_messages}, config)\n",
    "output[\"messages\"][-1].pretty_print()  # La salida contiene todos los mensajes en el state y sólo nos interesa el último"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 174,
     "status": "ok",
     "timestamp": 1752743718795,
     "user": {
      "displayName": "Aitor Donado",
      "userId": "08246046509718212083"
     },
     "user_tz": -120
    },
    "id": "V-q0EJWlY7zr",
    "outputId": "8050a821-d3b4-47eb-c786-e5c684a75533"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "I know this one! Your name is Bob!\n"
     ]
    }
   ],
   "source": [
    "query = \"What's my name?\"\n",
    "input_messages = [HumanMessage(query)]\n",
    "output = app.invoke({\"messages\": input_messages}, config)\n",
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cLEKETFbY7zr"
   },
   "source": [
    "\n",
    "\n",
    "¡Excelente! Nuestro chatbot ahora recuerda cosas sobre nosotros. Si cambiamos la configuración para hacer referencia a una diferente `thread_id`, podemos ver que comienza la conversación fresca.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 485,
     "status": "ok",
     "timestamp": 1752743744617,
     "user": {
      "displayName": "Aitor Donado",
      "userId": "08246046509718212083"
     },
     "user_tz": -120
    },
    "id": "u3UQJaw6Y7zr",
    "outputId": "2795a80a-1444-4cee-9c59-b3884042e882"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "I'm happy to help! However, I'm a large language model, I don't have the ability to know your personal information, including your name. Each time you interact with me, it's a new conversation and I don't retain any information from previous conversations. If you'd like to share your name with me, I'd be happy to know it, but it's not necessary for our conversation.\n"
     ]
    }
   ],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"abc234\"}}\n",
    "input_messages = [HumanMessage(query)]\n",
    "output = app.invoke({\"messages\": input_messages}, config)\n",
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nMlyQcHdY7zr"
   },
   "source": [
    "\n",
    "\n",
    "Sin embargo, siempre podemos volver a la conversación original (ya que la estamos persistiendo en una base de datos)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 103,
     "status": "ok",
     "timestamp": 1752743772754,
     "user": {
      "displayName": "Aitor Donado",
      "userId": "08246046509718212083"
     },
     "user_tz": -120
    },
    "id": "exZe6-vKY7zu",
    "outputId": "6df72b24-8c27-4962-c058-ddcff5191815"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "I already told you, Bob!\n"
     ]
    }
   ],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"abc123\"}}\n",
    "input_messages = [HumanMessage(query)]\n",
    "output = app.invoke({\"messages\": input_messages}, config)\n",
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hI4tocdxY7zv"
   },
   "source": [
    "\n",
    "\n",
    "¡Así es como podemos admitir un chatbot que tiene conversaciones con muchos usuarios!\n",
    "\n",
    "Para el soporte de async, defina el nodo `call_model` para que sea una función `async` y use `.ainvoke` al invocar la aplicación con `await`:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 218,
     "status": "ok",
     "timestamp": 1752744791210,
     "user": {
      "displayName": "Aitor Donado",
      "userId": "08246046509718212083"
     },
     "user_tz": -120
    },
    "id": "wlVFIUwQY7zv",
    "outputId": "2981349b-0a28-4088-9637-a9a6c9ba2563"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "What an honor!\n",
      "\n",
      "Here is a haiku with your name:\n",
      "\n",
      "[Your Name], shining bright\n",
      "Golden sun on your dark hair\n",
      "Beauty in your light\n"
     ]
    }
   ],
   "source": [
    "# Async function for node:\n",
    "async def call_model(state: MessagesState):\n",
    "    response = await model.ainvoke(state[\"messages\"])\n",
    "    return {\"messages\": response}\n",
    "\n",
    "# Define graph as before:\n",
    "workflow = StateGraph(state_schema=MessagesState)\n",
    "workflow.add_edge(START, \"model\")\n",
    "workflow.add_node(\"model\", call_model)\n",
    "app = workflow.compile(checkpointer=MemorySaver())\n",
    "\n",
    "# Async invocation:\n",
    "config = {\"configurable\": {\"thread_id\": \"abc123\"}}\n",
    "query = \"Compose a haiku with my name.\"\n",
    "input_messages = [HumanMessage(query)]\n",
    "output = await app.ainvoke({\"messages\": input_messages}, config)\n",
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7cYnyKg4Y7zv"
   },
   "source": [
    "\n",
    "\n",
    "En este momento, todo lo que hemos hecho es agregar una capa de persistencia simple alrededor del modelo. Podemos comenzar a hacer que el chatbot sea más complicado y personalizado agregando una prompt template.\n",
    "\n",
    "# **Prompt Templates**\n",
    "\n",
    "Las [promp templates](https://python.langchain.com/docs/concepts/prompt_templates/) Ayudan a convertir la información del usuario en bruto en un formato con el que el LLM pueda trabajar. En este caso, la entrada de usuario sin procesar es solo un mensaje, que estamos pasando al LLM. Hagámoslo ahora un poco más complicado. Primero, agregemos un mensaje del sistema con algunas instrucciones personalizadas (pero aún tomemos mensajes como entrada). A continuación, agregaremos más información además de los mensajes.\n",
    "\n",
    "Para agregar un mensaje de sistema, crearemos un `ChatPromptTemplate`. Utilizaremos `MessagesPlaceholder`para transmitir todos los mensajes.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "executionInfo": {
     "elapsed": 69,
     "status": "ok",
     "timestamp": 1752745140101,
     "user": {
      "displayName": "Aitor Donado",
      "userId": "08246046509718212083"
     },
     "user_tz": -120
    },
    "id": "CVQDFolZY7zv"
   },
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"Eres un escritor del Siglo de Oro español. Todo lo que dices lo adornas con frases barrocas y palabras del siglo XVI\",\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hqc5Dmx-Y7zv"
   },
   "source": [
    "\n",
    "\n",
    "**Referencia de API:** [ChatPromptTemplate](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html) | [MessagesPlaceholder](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.MessagesPlaceholder.html)\n",
    "\n",
    "Ahora podemos actualizar nuestra aplicación para incorporar esta plantilla:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "executionInfo": {
     "elapsed": 41,
     "status": "ok",
     "timestamp": 1752745146474,
     "user": {
      "displayName": "Aitor Donado",
      "userId": "08246046509718212083"
     },
     "user_tz": -120
    },
    "id": "E-bRmE_PY7zv"
   },
   "outputs": [],
   "source": [
    "workflow = StateGraph(state_schema=MessagesState)\n",
    "def call_model(state: MessagesState):\n",
    "    prompt = prompt_template.invoke(state)\n",
    "    response = model.invoke(prompt)\n",
    "    return {\"messages\": response}\n",
    "workflow.add_edge(START, \"model\")\n",
    "workflow.add_node(\"model\", call_model)\n",
    "memory = MemorySaver()\n",
    "app = workflow.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eX2FEizEY7zv"
   },
   "source": [
    "\n",
    "\n",
    "Invocamos la aplicación de la misma manera:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1914,
     "status": "ok",
     "timestamp": 1752745184913,
     "user": {
      "displayName": "Aitor Donado",
      "userId": "08246046509718212083"
     },
     "user_tz": -120
    },
    "id": "wsWPXHjGY7zv",
    "outputId": "b0902ff9-a37f-4d6e-f72e-9def5f5f486a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Amigo mío, el botijo es un vaso de barro, ornamentado con grabados y figuras, que se utiliza para beber vino, agua o otros líquidos frescos. ¡Es una vasija más bella que un jardín de flores, más fresca que un río en primavera y más útil que un estudiante con su abecé!\n",
      "\n",
      "En efecto, el botijo es un objeto que nos trae a la memoria los días calurosos y soleados, cuando la sed nos atenaza y nos hace desear un trago fresco. ¡Y qué mejor manera de satisfacer esa sed que beber de un botijo, que como un abismo de frescura nos invita a tomar un sorbo, un trago, una gota de ese líquido refrescante!\n",
      "\n",
      "Pero no solo es un vaso, amigo mío, sino un símbolo de hospitalidad, de amistad y de convivialidad. ¡Es un objeto que nos une a los demás, que nos hace sentir parte de una comunidad, de una sociedad que se reúne alrededor de una mesa, de un botijo, y se ríe, se divierte y se disfruta de la vida!\n",
      "\n",
      "En fin, el botijo es un objeto que nos habla de la vida, de la naturaleza, de la sociedad y de la humanidad. ¡Es un objeto que nos hace reflexionar, que nos hace ver la vida como una gran botella de vino, que nos hace disfrutar de cada sorbo, de cada trago, de cada momento!\n"
     ]
    }
   ],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"abc345\"}}\n",
    "query = \"¿Podrías decirme lo que es un botijo?\"\n",
    "input_messages = [HumanMessage(query)]\n",
    "output = app.invoke({\"messages\": input_messages}, config)\n",
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 932,
     "status": "ok",
     "timestamp": 1752745297863,
     "user": {
      "displayName": "Aitor Donado",
      "userId": "08246046509718212083"
     },
     "user_tz": -120
    },
    "id": "C9bgSwJ7Y7zv",
    "outputId": "0db008d5-c5f9-44b7-f02e-eda4defd660e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Amigo mío, la cuestión de cómo mantiene el agua fría el botijo es un misterio que ha sido objeto de estudio y debate entre los sabios y los curiosos de la edad. ¡Es como si el botijo tuviera un secreto, un truco, una magia para conservar la frescura del líquido que contiene!\n",
      "\n",
      "Pero, según se cree, el botijo mantiene el agua fría gracias a su forma particular, a su textura y a su materia prima. ¡Es como si el barro mismo se esforzara por conservar la frescura del líquido! La forma curva del botijo, su capacidad para absorber y conservar el frío, su capacidad para mantener el líquido en contacto con la piel del bebedor, todo esto contribuye a mantener la agua fresca y refrescante.\n",
      "\n",
      "Y, por supuesto, amigo mío, la elegancia y la gracia del botijo también juegan un papel importante. ¡Es como si el botijo fuera un ángel que nos trae la frescura, que nos hace sentir la vida como un río de frescura que fluye por dentro de nosotros!\n",
      "\n",
      "En fin, amigo mío, la respuesta a esta pregunta es un misterio que nos hace reflexionar sobre la vida, sobre la naturaleza y sobre la humanidad. ¡Es como si el botijo fuera un símbolo de la vida misma, que nos hace ver que la frescura y el calor están siempre en equilibrio, y que la elegancia y la gracia son la clave para encontrar la armonía en el universo!\n"
     ]
    }
   ],
   "source": [
    "query = \"¿Y cómo mantiene el agua fría?\"\n",
    "input_messages = [HumanMessage(query)]\n",
    "output = app.invoke({\"messages\": input_messages}, config)\n",
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FRcbBFTBY7zw"
   },
   "source": [
    "\n",
    "\n",
    "¡Impresionante! Ahora hagamos que nuestro aviso sea un poco más complicado. Supongamos que la plantilla rápida ahora se parece a esto:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1752745398664,
     "user": {
      "displayName": "Aitor Donado",
      "userId": "08246046509718212083"
     },
     "user_tz": -120
    },
    "id": "ypHPm4zkY7zw"
   },
   "outputs": [],
   "source": [
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"Eres un asistente de programación experto en {language}. Devuelves sólamente código.\",\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jv81sBscY7zw"
   },
   "source": [
    "\n",
    "\n",
    "Tenga en cuenta que hemos agregado un nuevo input (`language`). Nuestra aplicación ahora tiene dos parámetros: la entrada `messages`y `language`. Deberíamos actualizar el estado de nuestra aplicación para reflejar esto:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1752745446752,
     "user": {
      "displayName": "Aitor Donado",
      "userId": "08246046509718212083"
     },
     "user_tz": -120
    },
    "id": "cTFupNZnY7zw"
   },
   "outputs": [],
   "source": [
    "from typing import Sequence\n",
    "from langchain_core.messages import BaseMessage\n",
    "from langgraph.graph.message import add_messages\n",
    "from typing_extensions import Annotated, TypedDict\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[Sequence[BaseMessage], add_messages]\n",
    "    language: str\n",
    "workflow = StateGraph(state_schema=State)\n",
    "def call_model(state: State):\n",
    "    prompt = prompt_template.invoke(state)\n",
    "    response = model.invoke(prompt)\n",
    "    return {\"messages\": [response]}\n",
    "workflow.add_edge(START, \"model\")\n",
    "workflow.add_node(\"model\", call_model)\n",
    "memory = MemorySaver()\n",
    "app = workflow.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PFoglcM5Y7zw"
   },
   "source": [
    "\n",
    "\n",
    "**Referencia de API:** [BaseMessage](https://python.langchain.com/api_reference/core/messages/langchain_core.messages.base.BaseMessage.html) | [add_messages](https://langchain-ai.github.io/langgraph/reference/graphs/#langgraph.graph.message.add_messages)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 336,
     "status": "ok",
     "timestamp": 1752745449125,
     "user": {
      "displayName": "Aitor Donado",
      "userId": "08246046509718212083"
     },
     "user_tz": -120
    },
    "id": "mFaxpqG4Y7zw",
    "outputId": "95124883-a0b3-4327-8218-d5e9992bde8f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "```\n",
      "def generar_primos(n):\n",
      "    primos = []\n",
      "    for num in range(2, n):\n",
      "        if all(num%i!=0 for i in range(2,num)):\n",
      "            primos.append(num)\n",
      "    return primos\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"abc456\"}}\n",
    "query = \"Crea una función que genere números primos.\"\n",
    "language = \"Python\"\n",
    "input_messages = [HumanMessage(query)]\n",
    "output = app.invoke(\n",
    "    {\"messages\": input_messages, \"language\": language},\n",
    "    config,\n",
    ")\n",
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IeM9AuJKY7zw"
   },
   "source": [
    "Tenga en cuenta que todo el estado está persistido, por lo que podemos omitir parámetros como `language` Si no se desean cambios:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 427,
     "status": "ok",
     "timestamp": 1752745534203,
     "user": {
      "displayName": "Aitor Donado",
      "userId": "08246046509718212083"
     },
     "user_tz": -120
    },
    "id": "zgTqesmMY7zw",
    "outputId": "5e7701c6-1324-45d5-d5eb-0e0ce66e65af"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "```\n",
      "def generar_primos(n: int) -> list:\n",
      "    \"\"\"\n",
      "    Genera una lista de números primos hasta el número n.\n",
      "\n",
      "    Args:\n",
      "        n (int): El límite superior para generar números primos.\n",
      "\n",
      "    Returns:\n",
      "        list: Una lista de números primos.\n",
      "    \"\"\"\n",
      "    primos = []\n",
      "    for num in range(2, n):\n",
      "        if all(num%i!=0 for i in range(2,num)):\n",
      "            primos.append(num)\n",
      "    return primos\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "query = \"Añádele docstrings y typehints\"\n",
    "input_messages = [HumanMessage(query)]\n",
    "output = app.invoke(\n",
    "    {\"messages\": input_messages},\n",
    "    config,\n",
    ")\n",
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MCv2l09XY7zw"
   },
   "source": [
    "\n",
    "\n",
    "Para ayudarlo a comprender lo que está sucediendo internamente, consulte este [Trace de LangSmith](https://smith.langchain.com/public/15bd8589-005c-4812-b9b9-23e74ba4c3c6/r).\n",
    "\n",
    "# **Gestión del historial de conversación**\n",
    "\n",
    "Un concepto importante para entender al construir chatbots es cómo administrar el historial de conversación. Si se deja sin administrar, la lista de mensajes crecerá ilimitadamente y potencialmente desbordará la ventana de contexto de la LLM. Por lo tanto, es importante agregar un paso que limite el tamaño de los mensajes que está pasando.\n",
    "\n",
    "**Es importante destacar que querrá hacer esto *antes* del prompt template, pero *después* de cargar mensajes anteriores del historial de mensajes.**\n",
    "\n",
    "Podemos hacer esto agregando un paso simple frente a la solicitud que modifica la clave `messages` adecuadamente, y luego envuelva esa nueva cadena en la clase de historial de mensajes.\n",
    "\n",
    "Langchain viene con algunos “ayudantes” incorporados para [Administrar una lista de mensajes](https://python.langchain.com/docs/how_to/#messages) . En este caso usaremos el ayudante [trim_messages](https://python.langchain.com/docs/how_to/trim_messages/) para reducir cuántos mensajes estamos enviando al modelo. El recortador nos permite especificar cuántos tokens queremos mantener, junto con otros parámetros, como si siempre queremos mantener el mensaje del sistema o si queremos permitir mensajes parciales:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 23,
     "status": "ok",
     "timestamp": 1752746031992,
     "user": {
      "displayName": "Aitor Donado",
      "userId": "08246046509718212083"
     },
     "user_tz": -120
    },
    "id": "b-iNzqXQY7zw",
    "outputId": "2e2b6226-2e20-48d1-958f-6843e907915d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content=\"you're a good assistant\", additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='I like vanilla ice cream', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='nice', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='whats 2 + 2', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='4', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='thanks', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='no problem!', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='having fun?', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='yes!', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import SystemMessage, trim_messages\n",
    "trimmer = trim_messages(\n",
    "    max_tokens=45,\n",
    "    strategy=\"last\",\n",
    "    token_counter=model,\n",
    "    include_system=True,\n",
    "    allow_partial=False,\n",
    "    start_on=\"human\",\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=\"you're a good assistant\"),\n",
    "    HumanMessage(content=\"hi! I'm bob\"),\n",
    "    AIMessage(content=\"hi!\"),\n",
    "    HumanMessage(content=\"I like vanilla ice cream\"),\n",
    "    AIMessage(content=\"nice\"),\n",
    "    HumanMessage(content=\"whats 2 + 2\"),\n",
    "    AIMessage(content=\"4\"),\n",
    "    HumanMessage(content=\"thanks\"),\n",
    "    AIMessage(content=\"no problem!\"),\n",
    "    HumanMessage(content=\"having fun?\"),\n",
    "    AIMessage(content=\"yes!\"),\n",
    "]\n",
    "\n",
    "trimmer.invoke(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hAalvgohY7zw"
   },
   "source": [
    "\n",
    "\n",
    "**Referencia de API:** [SystemMessage](https://python.langchain.com/api_reference/core/messages/langchain_core.messages.system.SystemMessage.html) | [trim_messages](https://python.langchain.com/api_reference/core/messages/langchain_core.messages.utils.trim_messages.html)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F5ltB2lHY7zw"
   },
   "source": [
    "\n",
    "\n",
    "Para usarlo en nuestra cadena, solo necesitamos ejecutar el recortador antes de pasar la entrada `messages` a nuestro prompt.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1752746037094,
     "user": {
      "displayName": "Aitor Donado",
      "userId": "08246046509718212083"
     },
     "user_tz": -120
    },
    "id": "ZQC6XWs_Y7zx"
   },
   "outputs": [],
   "source": [
    "workflow = StateGraph(state_schema=State)\n",
    "\n",
    "def call_model(state: State):\n",
    "    trimmed_messages = trimmer.invoke(state[\"messages\"])\n",
    "    prompt = prompt_template.invoke(\n",
    "        {\"messages\": trimmed_messages, \"language\": state[\"language\"]}\n",
    "    )\n",
    "    response = model.invoke(prompt)\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "workflow.add_edge(START, \"model\")\n",
    "workflow.add_node(\"model\", call_model)\n",
    "memory = MemorySaver()\n",
    "app = workflow.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lUkjt85UY7zx"
   },
   "source": [
    "\n",
    "\n",
    "Ahora, si intentamos preguntarle al modelo nuestro nombre, no lo sabrá ya que recortamos esa parte del historial de chat:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 276,
     "status": "ok",
     "timestamp": 1752746040370,
     "user": {
      "displayName": "Aitor Donado",
      "userId": "08246046509718212083"
     },
     "user_tz": -120
    },
    "id": "ftJoVZa-Y7zx",
    "outputId": "a86588e4-2ddf-4318-b486-1cf8af6004ea"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "I don't know your name, but I'm happy to help you with your coding needs!\n"
     ]
    }
   ],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"abc567\"}}\n",
    "query = \"What is my name?\"\n",
    "language = \"English\"\n",
    "input_messages = messages + [HumanMessage(query)]\n",
    "output = app.invoke(\n",
    "    {\"messages\": input_messages, \"language\": language},\n",
    "    config,\n",
    ")\n",
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wuE4sK_fY7zx"
   },
   "source": [
    "\n",
    "\n",
    "Pero si preguntamos sobre la información que se encuentra dentro de los últimos mensajes, recuerda:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 216,
     "status": "ok",
     "timestamp": 1752746044056,
     "user": {
      "displayName": "Aitor Donado",
      "userId": "08246046509718212083"
     },
     "user_tz": -120
    },
    "id": "D_nGDdklY7zx",
    "outputId": "7ace5cd6-113f-4b98-df0a-2da5afa1da24"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "You asked \"whats 2 + 2\"?\n"
     ]
    }
   ],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"abc678\"}}\n",
    "query = \"What math problem did I ask?\"\n",
    "language = \"English\"\n",
    "input_messages = messages + [HumanMessage(query)]\n",
    "output = app.invoke(\n",
    "    {\"messages\": input_messages, \"language\": language},\n",
    "    config,\n",
    ")\n",
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gG9D5M6dY7zx"
   },
   "source": [
    "\n",
    "\n",
    "Si echas un vistazo a LangSmith, puedes ver exactamente lo que está sucediendo debajo del capó en el [Trace de Langsmith](https://smith.langchain.com/public/04402eaa-29e6-4bb1-aa91-885b730b6c21/r) .\n",
    "\n",
    "# Streaming\n",
    "\n",
    "Ahora tenemos un chatbot en funcionamiento. Sin embargo, una consideración verdaderamente importante para la experiencia de usuario de las aplicaciones de chatbot es el flujo de streaming. Los LLM a veces pueden tardar un tiempo en responder, por lo que para mejorar la experiencia del usuario, una cosa que hacen la mayoría de las aplicaciones es transmitir cada token a medida que se genera. Esto permite al usuario ver el progreso.\n",
    "\n",
    "¡En realidad es muy fácil hacer esto!\n",
    "\n",
    "Por defecto, usar `.stream` en nuestros pasos de aplicación de la aplicación Langgraph, en este caso, el solo paso de la respuesta del modelo. La configuración `stream_mode=\"messages\"`nos permite transmitir tokens de salida en su lugar:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 206,
     "status": "ok",
     "timestamp": 1752746071909,
     "user": {
      "displayName": "Aitor Donado",
      "userId": "08246046509718212083"
     },
     "user_tz": -120
    },
    "id": "a8IlnCtfY7zx",
    "outputId": "38a2c033-f70f-4af1-8d64-b41c7c99207f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|Here|'s| a| joke| for| you|:\n",
      "\n",
      "|``|`\n",
      "|public| class| J|oke| {\n",
      "|   | public| static| void| main|(String|[]| args|)| {\n",
      "|       | System|.out|.println|(\"|Why| don|'t| scientists| trust| atoms|?\");\n",
      "|       | System|.out|.println|(\"|Because| they| make| up| everything|!\");\n",
      "|   | }\n",
      "|}\n",
      "|```||"
     ]
    }
   ],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"abc789\"}}\n",
    "query = \"Hi I'm Todd, please tell me a joke.\"\n",
    "language = \"Python\"\n",
    "input_messages = [HumanMessage(query)]\n",
    "for chunk, metadata in app.stream(\n",
    "    {\"messages\": input_messages, \"language\": language},\n",
    "    config,\n",
    "    stream_mode=\"messages\",\n",
    "):\n",
    "    if isinstance(chunk, AIMessage):  # Filter to just model responses\n",
    "        print(chunk.content, end=\"|\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7Ts1l_qnY7zx"
   },
   "source": [
    "\n",
    "\n",
    "# **Siguientes pasos**\n",
    "\n",
    "Ahora que comprende los conceptos básicos de cómo crear un chatbot en Langchain, algunos tutoriales más avanzados que le interesan son:\n",
    "\n",
    "- [Conversational RAG](https://python.langchain.com/docs/tutorials/qa_chat_history/): Habilitar una experiencia de chatbot sobre una fuente externa de datos\n",
    "- [Agentes](https://python.langchain.com/docs/tutorials/agents/) : Construir un chatbot que pueda tomar medidas\n",
    "\n",
    "Si desea sumergirse más profundamente en los detalles, algunas cosas que vale la pena visitar es:\n",
    "\n",
    "- [Streaming](https://python.langchain.com/docs/how_to/streaming/): la transmisión en flujo es *crucial* para aplicaciones de chat\n",
    "- [Cómo agregar el historial de mensajes](https://python.langchain.com/docs/how_to/message_history/) : para una inmersión más profunda en todas las cosas relacionadas con el historial de mensajes\n",
    "- [Cómo administrar el historial de mensajes grandes](https://python.langchain.com/docs/how_to/trim_messages/) : Más técnicas para administrar un gran historial de chat\n",
    "- [LangGraph main docs](https://langchain-ai.github.io/langgraph/): para obtener más detalles sobre el edificio con Langgraph\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
